[
    {
      "id": "2308.14025",
      "title": "SSRL: Self-Supervised Reinforcement Learning for Language Agents",
      "source": "ARXIV",
      "type": "PAPER",
      "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar"],
      "confidence": 0.92,
      "citations": 187,
      "tags": ["RLHF", "self-supervised", "language agents", "reinforcement learning"],
      "published": "2023-08-25",
      "abstract": "We present SSRL, a novel approach to self-supervised reinforcement learning for language agents that combines contrastive learning with policy optimization. Our method enables agents to learn from unlabeled interaction traces while maintaining alignment with human preferences.",
      "doi": "10.48550/arXiv.2308.14025",
      "venue": "arXiv preprint",
      "keywords": ["self-supervised learning", "reinforcement learning", "language models", "agent training"],
      "url": "https://arxiv.org/abs/2308.14025",
      "references": ["1706.03762", "2203.02155", "2204.05862"]
    },
    {
      "id": "openpipe/art",
      "title": "Agent Reinforcement Trainer",
      "source": "GITHUB",
      "type": "CODE",
      "authors": ["OpenPipe Team", "Kyle Corbitt", "David Zhang"],
      "confidence": 0.88,
      "citations": 42,
      "tags": ["agent training", "GRPO", "LangGraph", "reinforcement learning", "python"],
      "published": "2024-07-10",
      "description": "A comprehensive toolkit for training reinforcement learning agents using GRPO (Group Relative Policy Optimization) with LangGraph integration. Supports multi-agent environments and preference learning.",
      "url": "https://github.com/openpipe/art",
      "language": "Python",
      "stars": 1247,
      "forks": 89,
      "topics": ["reinforcement-learning", "agent-training", "langgraph", "grpo"],
      "references": ["langchain/langgraph", "huggingface/transformers"]
    },
    {
      "id": "1706.03762",
      "title": "Attention Is All You Need",
      "source": "ARXIV",
      "type": "PAPER",
      "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"],
      "confidence": 0.98,
      "citations": 89247,
      "tags": ["transformers", "attention", "neural networks", "NLP", "foundational"],
      "published": "2017-06-12",
      "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.",
      "doi": "10.48550/arXiv.1706.03762",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
      "keywords": ["attention mechanism", "transformer", "sequence modeling", "neural machine translation"],
      "url": "https://arxiv.org/abs/1706.03762",
      "references": ["1409.0473", "1508.04025", "1511.06391"]
    },
    {
      "id": "2203.02155",
      "title": "Training language models to follow instructions with human feedback",
      "source": "ARXIV",
      "type": "PAPER",
      "authors": ["Long Ouyang", "Jeff Wu", "Xu Jiang", "Diogo Almeida", "Carroll L. Wainwright", "Pamela Mishkin", "Chong Zhang", "Sandhini Agarwal", "Katarina Slama", "Alex Ray", "John Schulman", "Jacob Hilton", "Fraser Kelton", "Luke Miller", "Maddie Simens", "Amanda Askell", "Peter Welinder", "Paul Christiano", "Jan Leike", "Ryan Lowe"],
      "confidence": 0.95,
      "citations": 4521,
      "tags": ["RLHF", "instruction following", "human feedback", "alignment", "GPT"],
      "published": "2022-03-04",
      "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback.",
      "doi": "10.48550/arXiv.2203.02155",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)",
      "keywords": ["reinforcement learning from human feedback", "instruction tuning", "alignment", "language models"],
      "url": "https://arxiv.org/abs/2203.02155",
      "references": ["1706.03762", "2005.14165", "2009.01325"]
    },
    {
      "id": "langchain/langgraph",
      "title": "LangGraph: Multi-Agent Workflows",
      "source": "GITHUB",
      "type": "CODE",
      "authors": ["LangChain Team", "Harrison Chase", "Ankush Gola"],
      "confidence": 0.91,
      "citations": 156,
      "tags": ["multi-agent", "workflows", "langchain", "graph", "orchestration"],
      "published": "2024-01-15",
      "description": "LangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence.",
      "url": "https://github.com/langchain-ai/langgraph",
      "language": "Python",
      "stars": 8934,
      "forks": 1456,
      "topics": ["langchain", "multi-agent", "workflows", "llm", "graph"],
      "references": ["langchain/langchain", "microsoft/autogen"]
    },
    {
      "id": "semantic_scholar_dataset",
      "title": "The Semantic Scholar Open Research Corpus",
      "source": "SEMANTIC_SCHOLAR",
      "type": "DATASET",
      "authors": ["Semantic Scholar Team", "Waleed Ammar", "Dirk Groeneveld"],
      "confidence": 0.89,
      "citations": 892,
      "tags": ["dataset", "research corpus", "citations", "academic papers"],
      "published": "2018-09-27",
      "abstract": "We release the Semantic Scholar Open Research Corpus (S2ORC), a large corpus of 81.1M English-language academic papers spanning many academic disciplines. The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers.",
      "doi": "10.18653/v1/2020.acl-main.447",
      "venue": "Association for Computational Linguistics (ACL)",
      "keywords": ["academic corpus", "citation network", "research dataset", "NLP"],
      "url": "https://www.semanticscholar.org/paper/Construction-of-the-Literature-Graph-in-Semantic-Ammar-Groeneveld/649def34f8be52c8b66281af98ae884c09aef38b",
      "references": ["1706.03762", "2203.02155"]
    },
    {
      "id": "wikipedia_transformers",
      "title": "Transformer (machine learning model)",
      "source": "WIKIPEDIA",
      "type": "CONCEPT",
      "authors": ["Wikipedia Contributors"],
      "confidence": 0.82,
      "citations": 0,
      "tags": ["transformers", "machine learning", "encyclopedia", "concept"],
      "published": "2023-12-15",
      "abstract": "A transformer is a deep learning architecture that relies on the parallel multi-head attention mechanism. The transformer architecture has been adopted as the building block of most modern NLP architectures, including BERT and GPT.",
      "url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)",
      "keywords": ["transformer", "attention mechanism", "deep learning", "NLP"],
      "references": ["1706.03762", "1810.04805"]
    },
    {
      "id": "huggingface/transformers",
      "title": "Transformers: State-of-the-art Machine Learning for PyTorch, TensorFlow, and JAX",
      "source": "GITHUB",
      "type": "CODE",
      "authors": ["Hugging Face Team", "Thomas Wolf", "Lysandre Debut", "Victor Sanh"],
      "confidence": 0.96,
      "citations": 234,
      "tags": ["transformers", "pytorch", "tensorflow", "jax", "NLP", "library"],
      "published": "2019-10-09",
      "description": "ðŸ¤— Transformers: State-of-the-art Machine Learning for PyTorch, TensorFlow, and JAX. Provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.",
      "url": "https://github.com/huggingface/transformers",
      "language": "Python",
      "stars": 132847,
      "forks": 26394,
      "topics": ["transformers", "pytorch", "tensorflow", "jax", "nlp", "computer-vision", "audio"],
      "references": ["1706.03762", "1810.04805", "2005.14165"]
    }
  ]
  